{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d754bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability Comparison of All Tokens\n",
      "           Token  Laplace Probability  KneserNey Probability\n",
      "0              $           0.00012771             0.00023928\n",
      "1              i           0.26934866             0.87810280\n",
      "2          stand           0.00012771             0.00000156\n",
      "3           here           0.00012771             0.00000623\n",
      "4           feel           0.00012771             0.00002181\n",
      "...          ...                  ...                    ...\n",
      "5425      google           0.00012771             0.00000019\n",
      "5426  stellarium           0.00012771             0.00000019\n",
      "5427       theyd           0.00012771             0.00000019\n",
      "5428       peter           0.00012771             0.00000019\n",
      "5429      robbed           0.00012771             0.00000019\n",
      "\n",
      "[5430 rows x 3 columns]\n",
      "\n",
      "Probability Comparison of Next Available Tokens in Bigram Dictionary\n",
      "        Token  Laplace Probability  KneserNey Probability  Bigram Probability\n",
      "1           i           0.26934866             0.87810280          0.87833333\n",
      "6           a           0.00051086             0.00098247          0.00291667\n",
      "40       when           0.00114943             0.00305082          0.08833333\n",
      "58         in           0.00025543             0.00017153          0.01791667\n",
      "75        the           0.00051086             0.00100078          0.00041667\n",
      "106       ill           0.00102171             0.00260709          0.00333333\n",
      "135      this           0.00025543             0.00013084          0.00125000\n",
      "140        im           0.02720307             0.08803368          0.00041667\n",
      "156    during           0.00025543             0.00010611          0.00041667\n",
      "178        is           0.00025543             0.00013473          0.00041667\n",
      "227        on           0.00025543             0.00014019          0.00125000\n",
      "237        no           0.00025543             0.00011176          0.00041667\n",
      "400       ive           0.00561941             0.01761001          0.00333333\n",
      "494        id           0.00114943             0.00302375          0.00041667\n",
      "4111   heated           0.00025543             0.00010436          0.00041667\n",
      "4489  occured           0.00025543             0.00010436          0.00041667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BigramLM:\n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.bigramDictionary = defaultdict(int)\n",
    "        self.bigramProbabilities = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        self.discount = 0.75               # Kneser Ney Discount Factor\n",
    "        self.probability_comparison_frame = pd.DataFrame()\n",
    "        self.probability_comparison_frame_available = pd.DataFrame()\n",
    "\n",
    "    def learn_model(self, dataset):\n",
    "        for sentence in dataset:\n",
    "            tokens = sentence\n",
    "            tokens.append('$')  # Adding End of Sentence marker\n",
    "            for i in range(len(tokens)):\n",
    "                prev_token = tokens[i - 1]\n",
    "                current_token = tokens[i]\n",
    "\n",
    "                self.bigram_counts[prev_token][current_token] += 1\n",
    "                self.unigram_counts[prev_token] += 1\n",
    "                self.vocab.add(prev_token)\n",
    "                self.bigramDictionary[(prev_token, current_token)] += 1\n",
    "\n",
    "        self.calculate_probability()\n",
    "        \n",
    "    def calculate_probability(self):\n",
    "        for bigram in self.bigramDictionary:\n",
    "            self.bigramProbabilities[bigram] = (self.bigramDictionary[bigram]) / (self.unigram_counts[bigram[0]])\n",
    "            \n",
    "    def laplace_smoothing_probability(self, bigram):\n",
    "        prefix_count = self.unigram_counts[bigram[0]]\n",
    "        if bigram not in self.bigramDictionary:\n",
    "            return ((1)/(prefix_count+len(self.vocab))) \n",
    "        \n",
    "        bigram_count = self.bigramDictionary[bigram]\n",
    "        return ((bigram_count+1)/(prefix_count+len(self.vocab)))      \n",
    "        \n",
    "    def kneserney_smoothing_probability(self, bigram):\n",
    "        prefix_count = self.unigram_counts[bigram[0]]\n",
    "        bigram_count = 0\n",
    "        if bigram in self.bigramDictionary:\n",
    "            bigram_count = self.bigramDictionary[bigram]\n",
    "        \n",
    "        bigram_types_with_suffix = len([x for x in self.bigramDictionary if x[1]==bigram[1]]) # fixed suffix, variable prefix(wi-1)\n",
    "        bigram_types_with_prefix = len([x for x in self.bigramDictionary if x[0]==bigram[0]]) # fixed prefix, variable suffix(wi)\n",
    "        total_bigram_types = len(self.bigramDictionary)\n",
    "        \n",
    "        discounted_prob = max(bigram_count - self.discount, 0) / prefix_count\n",
    "        alpha_parameter = (self.discount / prefix_count) * bigram_types_with_prefix\n",
    "        pcontinuation = bigram_types_with_suffix / total_bigram_types\n",
    "        \n",
    "        return discounted_prob + alpha_parameter * pcontinuation\n",
    "            \n",
    "    def generate_next_token(self, prev_token):\n",
    "        if prev_token in self.bigram_counts:\n",
    "            next_tokens = list(self.bigram_counts[prev_token].keys())\n",
    "            probabilities = [self.bigramProbabilities[(prev_token, token)] for token in next_tokens]\n",
    "            next_token = np.random.choice(next_tokens, p=probabilities)\n",
    "            return next_token\n",
    "\n",
    "        return None\n",
    "\n",
    "    def generate_next_token_using_laplace(self, prev_token):\n",
    "        next_tokens = [token for token in self.unigram_counts]\n",
    "        \n",
    "        probabilities = np.array([self.laplace_smoothing_probability((prev_token, token)) for token in next_tokens])\n",
    "        \n",
    "        next_token = np.random.choice(next_tokens, p=probabilities)\n",
    "        return next_token\n",
    "\n",
    "    def generate_next_token_using_kneserney(self, prev_token):\n",
    "        next_tokens = [token for token in self.unigram_counts]\n",
    "        \n",
    "        probabilities = np.array([self.kneserney_smoothing_probability((prev_token, token)) for token in next_tokens])\n",
    "        next_token = np.random.choice(next_tokens, p=probabilities)\n",
    "        return next_token\n",
    "        \n",
    "    def generate_sentences_standard_bigram(self, num_samples, start_token=\"$\"):\n",
    "        if len(start_token.split()) ==0:\n",
    "            start_token = \"$\"\n",
    "            \n",
    "        start_token = start_token.split()[-1]\n",
    "        for x in range(num_samples):\n",
    "            prev_token = start_token\n",
    "            for i in range(10):\n",
    "                next_token = self.generate_next_token(prev_token)\n",
    "                print(prev_token, end=\" \")\n",
    "                prev_token = next_token\n",
    "            print()\n",
    "        print()\n",
    "        \n",
    "    def generate_sentences_laplace(self, num_samples, start_token=\"$\"):\n",
    "        if len(start_token.split()) ==0:\n",
    "            start_token = \"$\"\n",
    "            \n",
    "        start_token = start_token.split()[-1]\n",
    "        for x in range(num_samples):\n",
    "            prev_token = start_token\n",
    "            for i in range(10):\n",
    "                next_token = self.generate_next_token_using_laplace(prev_token)\n",
    "                print(prev_token, end=\" \")\n",
    "                prev_token = next_token\n",
    "            print()\n",
    "        print()\n",
    "        \n",
    "            \n",
    "    def generate_sentences_kneserney(self, num_samples, start_token=\"$\"):\n",
    "        if len(start_token.split()) ==0:\n",
    "            start_token = \"$\"\n",
    "            \n",
    "        start_token = start_token.split()[-1]\n",
    "        for x in range(num_samples):\n",
    "            prev_token = start_token\n",
    "            for i in range(10):\n",
    "                next_token = self.generate_next_token_using_kneserney(prev_token)\n",
    "                print(prev_token, end=\" \")\n",
    "                prev_token = next_token\n",
    "            print()\n",
    "        print()\n",
    "            \n",
    "    def compare_probabilities(self, prev_token=\"$\"):\n",
    "        if len(prev_token.split()) ==0:\n",
    "            prev_token = \"$\"\n",
    "            \n",
    "        prev_token = prev_token.split()[-1]\n",
    "        if prev_token in self.bigram_counts:\n",
    "            available_next_tokens = list(self.bigram_counts[prev_token].keys())\n",
    "            probabilities_bigram = [self.bigramProbabilities[(prev_token, token)] for token in available_next_tokens]\n",
    "            \n",
    "            all_tokens = np.array([token for token in self.unigram_counts])\n",
    "            probabilities_laplace = np.array([self.laplace_smoothing_probability((prev_token, token)) for token in self.unigram_counts])\n",
    "            probabilities_kneserney = np.array([self.kneserney_smoothing_probability((prev_token, token)) for token in self.unigram_counts])\n",
    "            \n",
    "#             print(\"Standard Bigram Model Probabilities\")\n",
    "#             print(probabilities_bigram)\n",
    "\n",
    "            # Set display options for float formatting\n",
    "            pd.set_option('display.float_format', '{:.8f}'.format)\n",
    "            # Create Probability DataFrame\n",
    "            self.probability_comparison_frame = pd.DataFrame({\n",
    "                'Token': all_tokens,\n",
    "                'Laplace Probability': probabilities_laplace,\n",
    "                'KneserNey Probability': probabilities_kneserney\n",
    "            })\n",
    "            \n",
    "            self.probability_comparison_frame_available = self.probability_comparison_frame[self.probability_comparison_frame[\"Token\"].isin(available_next_tokens)]\n",
    "            self.probability_comparison_frame_available[\"Bigram Probability\"] = probabilities_bigram\n",
    "            print(\"Probability Comparison of All Tokens\")\n",
    "            print(self.probability_comparison_frame)\n",
    "            \n",
    "            print(\"\\nProbability Comparison of Next Available Tokens in Bigram Dictionary\")\n",
    "            print(self.probability_comparison_frame_available)\n",
    "            \n",
    "#             print(\"\\nLaplace Bigram Model Probabilities\")\n",
    "#             print(list(probabilities_laplace))\n",
    "            \n",
    "#             print(\"\\nKneserNey Bigram Model Probabilities\")\n",
    "#             print(list(probabilities_kneserney))\n",
    "            return\n",
    "\n",
    "        return None\n",
    "\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = [line.strip().split() for line in file] \n",
    "\n",
    "bigram_model = BigramLM()\n",
    "bigram_model.learn_model(corpus)\n",
    "\n",
    "# bigram_model.generate_sentences_standard_bigram(\"of\", 10)\n",
    "\n",
    "# bigram_model.generate_sentences_laplace(10, \" \")\n",
    "\n",
    "# bigram_model.generate_sentences_kneserney(\"of\", 10)\n",
    "\n",
    "bigram_model.compare_probabilities()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d454a",
   "metadata": {},
   "source": [
    "## Argument:\n",
    "\n",
    "As we can see, their is a huge difference in Laplace probabilities and original Bigram probabilities. This is because Laplace smoothing steals a large amount of probabilities from non-zero counts of tokens to distribute it into tokens with zero occurrences. Thus in Laplace smoothing, the reconstructed counts of non-zero tokens could change largely sometimes by a factor of 10 from their original counts. For example, token \"i\" has a Laplace probability of 0.269 compared to its original probability of 0.87 i.e. a 1/3rd of difference from original probability while KneserNey has somewhat managed to maintain the probability in proportion with original probability.\n",
    "Thus Laplace haven't worked in our Bigram model well and doesn't work well in general for n-grams.\n",
    "\n",
    "On the other hand, Kneser-Ney has somewhat maintained the probabilities with original Bigram probabilities compared to Laplace.\n",
    "\n",
    "Thus KneserNey smoothing is better than Laplace in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adf8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
